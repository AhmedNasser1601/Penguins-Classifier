{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ae652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeHotEncoderForOutput(dataset, header):\n",
    "    # The process of making Encoder\n",
    "    headers = ['C1', 'C2', 'C3', 'X1', 'X2', 'X3', 'X4']\n",
    "    ct = ColumnTransformer([(header, OneHotEncoder(),\n",
    "                           [dataset.columns.get_loc(header)])],\n",
    "                           remainder='passthrough')\n",
    "    dataset = ct.fit_transform(dataset)\n",
    "    dataset = pd.DataFrame(dataset, columns=headers)\n",
    "\n",
    "    # the dataset after making Encoder\n",
    "    return dataset\n",
    "\n",
    "def inputDataPreprocessing(dataset):\n",
    "    # Get Classes Values then Shuffle it\n",
    "    class_a, class_b, class_c = dataset[:50], dataset[50:100], dataset[100:]\n",
    "    class_a, class_b, class_c = class_a.sample(frac=1), class_b.sample(frac=1), class_c.sample(frac=1)\n",
    "\n",
    "    # Then Split The data into 30 Train and 20 Test\n",
    "    class_a_train, class_a_test = class_a[:30], class_a[30:]\n",
    "    class_b_train, class_b_test = class_b[:30], class_b[30:]\n",
    "    class_c_train, class_c_test = class_c[:30], class_c[30:]\n",
    "\n",
    "    # Merge trained data together and tested data together\n",
    "    trained_data = pd.concat([class_a_train, class_b_train, class_c_train])\n",
    "    tested_data = pd.concat([class_a_test, class_b_test, class_c_test])\n",
    "\n",
    "    # Then return both\n",
    "    return trained_data, tested_data\n",
    "\n",
    "def SplitInputsAndOutput(dataset):\n",
    "    # Initialize matrices with suitable shapes\n",
    "    inputs_matrix = np.zeros([len(dataset), 5])  # 4 inputs + 1 bias\n",
    "    output_matrix = np.zeros([len(dataset), 3])\n",
    "\n",
    "    # Make Split Process\n",
    "    for i in range(len(dataset)):\n",
    "        output_matrix[i] = dataset[i][0:3]\n",
    "        inputs_matrix[i] = dataset[i][3:]\n",
    "\n",
    "    # Then return both\n",
    "    return inputs_matrix, output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b319323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSuitableWeight(hidden_layers, neurons_number):\n",
    "    # Create weight list that carry matrices\n",
    "    weights_list = list()\n",
    "\n",
    "    # According to number of layers create weight matrices\n",
    "    weights_list.append(np.random.randn(5, neurons_number[0]))\n",
    "    for i in range(hidden_layers - 1):\n",
    "        rows = neurons_number[i]\n",
    "        columns = neurons_number[i + 1]\n",
    "        weights_list.append(np.random.randn(rows, columns))\n",
    "    weights_list.append(np.random.randn(neurons_number[-1], 3))\n",
    "\n",
    "    # Return The list that included matrices of weights\n",
    "    return weights_list\n",
    "\n",
    "def WorkwithBias(data, bias_decision):\n",
    "    if bias_decision == 0:\n",
    "        # Then Add bias to Trained Data with value = 0\n",
    "        bias_vector = [0 for _ in range(len(data))]\n",
    "    else:\n",
    "        # Then Add bias to Trained Data with value = 1\n",
    "        bias_vector = [1 for _ in range(len(data))]\n",
    "\n",
    "    # Then add it to data input and return whole trained/tested data\n",
    "    data.insert(loc=3, column='bias', value=bias_vector)\n",
    "    return data\n",
    "\n",
    "def workWithBias_Hidden(biasDes, neuronsVal):\n",
    "    # if user don't need to use bias then\n",
    "    # let neuron that represents it be equal zero\n",
    "    if biasDes == 0:\n",
    "        for neuron in range(len(neuronsVal) - 1):\n",
    "            neuronsVal[neuron][0] = 0\n",
    "\n",
    "    # But if user need to use bias then\n",
    "    # let neuron that represents it be equal one\n",
    "    else:\n",
    "        for neuron in range(len(neuronsVal) - 1):\n",
    "            neuronsVal[neuron][0] = 1\n",
    "\n",
    "def CreateNeuronsMat(neurons):\n",
    "    # Define the used matrix (list of matrices)\n",
    "    neurons_Mat = list()\n",
    "    for neuron in neurons:\n",
    "        neurons_Mat.append(np.zeros(neuron))\n",
    "    neurons_Mat.append(np.zeros(3))  # as output consists of 3 neurons\n",
    "\n",
    "    # Then return it\n",
    "    return neurons_Mat\n",
    "\n",
    "def CalculateNetValue(single_input, weight):\n",
    "    # Calculate and return net value for given input\n",
    "    return single_input.dot(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(netVal):\n",
    "    # Calculate and return Sigmoid function for net value\n",
    "    return 1 / (1 + np.exp(-netVal))\n",
    "\n",
    "def HyperboicTangent(netVal):\n",
    "    # Calculate and return Hyperbolic tangent function for net value\n",
    "    #sinh = np.exp(netVal) - np.exp(-netVal)\n",
    "    #cosh = np.exp(netVal) + np.exp(-netVal)\n",
    "    #return sinh / cosh\n",
    "    return np.tanh(netVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetExpectedNeuronValue(inpVal, weight, function_to_use):\n",
    "    # Calculate net value and apply activation function based on user choice\n",
    "    netVal = CalculateNetValue(inpVal, weight)\n",
    "    if function_to_use == \"Sigmoid\":\n",
    "        expectVal = Sigmoid(netVal)\n",
    "    else:\n",
    "        expectVal = HyperboicTangent(netVal)\n",
    "\n",
    "    # Then return Calculated value\n",
    "    return expectVal\n",
    "\n",
    "def GetNetworkOutput(predicted_outputs):\n",
    "    # The process of making the output of the network understandable\n",
    "    # by making the max value in output layer be 1 and others be 0s\n",
    "    for output in predicted_outputs:\n",
    "        max_index = np.argmax(output)\n",
    "        for index in range(len(output)):\n",
    "            if index == max_index:\n",
    "                output[index] = 1\n",
    "            else:\n",
    "                output[index] = 0\n",
    "\n",
    "    # Then return it\n",
    "    return predicted_outputs\n",
    "\n",
    "def GetWeightShape(weights, index):\n",
    "    # Get shape of each weight matrix for update step\n",
    "    x_shape, y_shape = weights[index].shape\n",
    "    return x_shape, y_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STEP_A_Feedforward(single_input, weights, activ_function, hidden, neurons_output, useBias):\n",
    "    # For all hidden layers + output layer\n",
    "    for layer in range(0, hidden + 1):\n",
    "        if layer == 0:\n",
    "            # Get Neurons Value between input and first Hidden Layer\n",
    "            neurons_output[layer] = GetExpectedNeuronValue(single_input, weights[layer], activ_function)\n",
    "            continue\n",
    "\n",
    "        # Then Get Neurons Value for remaining Hidden layers included output layer\n",
    "        neurons_output[layer] = GetExpectedNeuronValue(neurons_output[layer - 1], weights[layer], activ_function)\n",
    "\n",
    "    # then let first neuron works as bias\n",
    "    workWithBias_Hidden(biasDes=useBias, neuronsVal=neurons_output)\n",
    "\n",
    "    # Return The list that carry each neuron output\n",
    "    return neurons_output\n",
    "\n",
    "def STEP_B_Backpropagate(actual_output, predicted_output, weights, neurons_values, local_gradient, hidden):\n",
    "    # Walk layer per layer till the input layer (from back to front)\n",
    "    for layer in range(hidden, -1, -1):\n",
    "        activ_fun_drev = neurons_values[layer] * (1 - neurons_values[layer])\n",
    "\n",
    "        # for Output Layer\n",
    "        if layer == hidden:\n",
    "            error = actual_output - predicted_output\n",
    "            local_gradient[layer] = error * activ_fun_drev\n",
    "            continue\n",
    "\n",
    "        # For hidden Layers\n",
    "        sumPart = local_gradient[layer + 1].dot(weights[layer + 1].transpose())\n",
    "        local_gradient[layer] = activ_fun_drev * sumPart\n",
    "\n",
    "    # Return The Local Minimum for all neurons\n",
    "    return local_gradient\n",
    "\n",
    "def STEP_C_UpdateWeights(sample_inp, weights, hidden_layers, local_gradient, neurons_out, eta):\n",
    "    for layer in range(hidden_layers + 1):\n",
    "        # update the weights of first layer as it depends on input from data\n",
    "        if layer == 0:\n",
    "            x_shape, y_shape = GetWeightShape(weights=weights, index=layer)\n",
    "            input_x_local = sample_inp.reshape(x_shape, 1).dot(local_gradient[layer].reshape(1, y_shape))\n",
    "            weights[layer] = weights[layer] + (eta * input_x_local)\n",
    "            continue\n",
    "\n",
    "        # then update other layers(hidden+output) as it there depends on input from previous neurons\n",
    "        x_shape, y_shape = GetWeightShape(weights=weights, index=layer)\n",
    "        input_x_local = neurons_out[layer - 1].reshape(x_shape, 1).dot(local_gradient[layer].reshape(1, y_shape))\n",
    "        weights[layer] = weights[layer] + (eta * input_x_local)\n",
    "\n",
    "    # Then return optimal weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackPropagationAlgorithm(inputs, output, weight, activation, epochs, eta, layers, neurons, useBias):\n",
    "    # Create List carry vector for each neuron output and Local Minimum per layer\n",
    "    neurons_output = CreateNeuronsMat(neurons)\n",
    "    localGrad = CreateNeuronsMat(neurons)\n",
    "\n",
    "    # Learning Start from here\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(inputs)):\n",
    "            # STEP 1: Apply Feedforward\n",
    "            neurons_output = STEP_A_Feedforward(single_input=inputs[i],\n",
    "                                                weights=weight,\n",
    "                                                activ_function=activation,\n",
    "                                                hidden=layers,\n",
    "                                                neurons_output=neurons_output,\n",
    "                                                useBias=useBias)\n",
    "\n",
    "            # STEP 2: Apply Backpropagation\n",
    "            localGrad = STEP_B_Backpropagate(actual_output=output[i],\n",
    "                                             predicted_output=neurons_output[-1],\n",
    "                                             weights=weight,\n",
    "                                             neurons_values=neurons_output,\n",
    "                                             local_gradient=localGrad,\n",
    "                                             hidden=layers)\n",
    "\n",
    "            # STEP 3: Update Weights\n",
    "            weight = STEP_C_UpdateWeights(sample_inp=inputs[i],\n",
    "                                          weights=weight,\n",
    "                                          hidden_layers=layers,\n",
    "                                          local_gradient=localGrad,\n",
    "                                          neurons_out=neurons_output,\n",
    "                                          eta=eta)\n",
    "\n",
    "    # After learning finished return optimal weights reached\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47444dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyTestScenario(inputs, weights, activ_function, hidden, neurons_output, useBias):\n",
    "    # test data by applying feedforward step using calculated optimal weights\n",
    "    predicted_output = list()\n",
    "    for i in range(len(inputs)):\n",
    "        total_neurons_values = STEP_A_Feedforward(single_input=inputs[i],\n",
    "                                                  weights=weights,\n",
    "                                                  activ_function=activ_function,\n",
    "                                                  hidden=hidden,\n",
    "                                                  neurons_output=neurons_output,\n",
    "                                                  useBias=useBias)\n",
    "        predicted_output.append(total_neurons_values[-1])\n",
    "\n",
    "    # Then Get The output for all tested data and return it\n",
    "    network_output = GetNetworkOutput(predicted_output)\n",
    "    return network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563be8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateConfusionMatrix(output_test, network_output):\n",
    "    # Defined Variables\n",
    "    confusion_matrix = np.zeros([3, 3])\n",
    "    flowers = ['setosa', 'versicolor', 'virginica']\n",
    "    columns = ['Class ID', 'Predicted', 'Actual', 'Status']\n",
    "    row = list()\n",
    "    test_status = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for index in range(len(output_test)):\n",
    "\n",
    "        # Building the Confusion Matrix\n",
    "        actual_index = np.argmax(output_test[index])\n",
    "        predicted_index = np.argmax(network_output[index])\n",
    "        confusion_matrix[actual_index][predicted_index] += 1\n",
    "\n",
    "        # Here for visualization part\n",
    "        classID = predicted_index + 1\n",
    "        className = flowers[predicted_index]\n",
    "        actualClass = flowers[actual_index]\n",
    "        if actual_index == predicted_index:\n",
    "            status = \"Matching\"\n",
    "        else:\n",
    "            status = \"Mismatching\"\n",
    "        row.append([classID, className, actualClass, status])\n",
    "\n",
    "    # Then Print status of each output\n",
    "    rows = pd.DataFrame(row, columns=columns)\n",
    "    test_status = test_status.append(rows, ignore_index=True)\n",
    "    print(\"------------------- Test Result --------------------\")\n",
    "    print(test_status)\n",
    "    print()\n",
    "\n",
    "    # print Confusion Matrix\n",
    "    classes = ['C1', 'C2', 'C3']\n",
    "    print(\"---------------- Confusion Matrix ------------------\")\n",
    "    matrix = pd.DataFrame(confusion_matrix, columns=classes, index=classes)\n",
    "    print(matrix)\n",
    "    print()\n",
    "\n",
    "    # Then print accuracy by [sum of diagonal / total sum] and for each class Too\n",
    "    accuracy = np.trace(confusion_matrix) / np.sum(confusion_matrix)\n",
    "    percentage = len(output_test) / len(classes)\n",
    "    print(\"------------------- Accuracy -----------------------\")\n",
    "    for i in range(len(classes)):\n",
    "        print(\"Accuracy for Class {}: {}\".format(i+1, confusion_matrix[i][i]/percentage))\n",
    "    print(\"Accuracy for whole network: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
